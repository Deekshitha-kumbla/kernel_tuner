{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Matrix multiplication\n",
    "\n",
    "Matrix multiplication is one of the most well-known linear algebra algorithms, and frequently used to demonstrate the high-performance computing capabilities of GPUs. As such, an example using matrix multiplication could not be left out. A naive CUDA kernel for a square matrix multiplication is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load matmul_naive.cu\n",
    "#define WIDTH 4096\n",
    "\n",
    "__global__ void matmul_kernel(float *C, float *A, float *B) {\n",
    "    int x = blockIdx.x * block_size_x + threadIdx.x;\n",
    "    int y = blockIdx.y * block_size_y + threadIdx.y;\n",
    "    float sum = 0.0;\n",
    "\n",
    "    for (int k=0; k<WIDTH; k++) {\n",
    "        sum += A[y*WIDTH+k] * B[k*WIDTH+x];\n",
    "    }\n",
    "\n",
    "    C[y*WIDTH+x] = sum;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel simply creates a single thread per output element. Each thread computes the index of the element it is responsible for, and iterates over the corresponding row in A, and corresponding column in B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: GeForce GTX TITAN X\n",
      "block_size_x=8, block_size_y=1, time=3349.53833008\n",
      "block_size_x=8, block_size_y=2, time=1678.16967773\n",
      "block_size_x=8, block_size_y=4, time=876.774719238\n",
      "block_size_x=8, block_size_y=8, time=861.712475586\n",
      "block_size_x=8, block_size_y=16, time=782.979882813\n",
      "block_size_x=8, block_size_y=32, time=605.516772461\n",
      "block_size_x=16, block_size_y=1, time=1706.25876465\n",
      "block_size_x=16, block_size_y=2, time=855.339624023\n",
      "block_size_x=16, block_size_y=4, time=763.935302734\n",
      "block_size_x=16, block_size_y=8, time=706.841027832\n",
      "block_size_x=16, block_size_y=16, time=585.361218262\n",
      "block_size_x=16, block_size_y=32, time=501.997399902\n",
      "block_size_x=32, block_size_y=1, time=985.078295898\n",
      "block_size_x=32, block_size_y=2, time=858.53651123\n",
      "block_size_x=32, block_size_y=4, time=817.795361328\n",
      "block_size_x=32, block_size_y=8, time=628.078552246\n",
      "block_size_x=32, block_size_y=16, time=555.53515625\n",
      "block_size_x=32, block_size_y=32, time=526.978662109\n",
      "block_size_x=64, block_size_y=1, time=964.572619629\n",
      "block_size_x=64, block_size_y=2, time=944.103857422\n",
      "block_size_x=64, block_size_y=4, time=792.807373047\n",
      "block_size_x=64, block_size_y=8, time=683.105700684\n",
      "block_size_x=64, block_size_y=16, time=604.543640137\n",
      "best performing configuration: block_size_x=16, block_size_y=32, time=501.997399902\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import numpy\n",
    "import kernel_tuner\n",
    "from collections import OrderedDict\n",
    "\n",
    "problem_size = (4096, 4096)\n",
    "size = numpy.prod(problem_size)\n",
    "\n",
    "A = numpy.random.randn(*problem_size).astype(numpy.float32)\n",
    "B = numpy.random.randn(*problem_size).astype(numpy.float32)\n",
    "C = numpy.zeros_like(A)\n",
    "\n",
    "args = [C, A, B]\n",
    "tune_params = OrderedDict()\n",
    "tune_params[\"block_size_x\"] = [2**i for i in range(3,7)]\n",
    "tune_params[\"block_size_y\"] = [2**i for i in range(6)]\n",
    "\n",
    "answer = [numpy.dot(A,B), None, None]\n",
    "\n",
    "results = kernel_tuner.tune_kernel(\"matmul_kernel\", \"matmul_naive.cu\",\n",
    "                                   problem_size, args, tune_params, answer=answer, atol=1e-3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aren't many parameters to tune yet, and more importantly, tuning will not be very effective because this kernel will be limited by bandwidth rather than compute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utilisation of the GPU is very low, even for the optimal combination of block_size_x and block_size_y:\n",
    "\n",
    "![](Matmul-naive-utilisation.png)\n",
    "\n",
    "There is however, a lot of opportunity for data reuse, which is realized by making the threads in a thread block collaborate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Increase data reuse\n",
    "\n",
    "This can be solved by using a technique called loop-blocking or loop-tiling. We define two square data structures in shared memory, which will be used for storing square parts of matrix A and B. The threads in a thread block will collaboratively fill these two variables, and then proceed to perform all the computations that need this data, before moving to the next blocked iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load matmul_data_reuse.cu\n",
    "#define WIDTH 4096\n",
    "\n",
    "__global__ void matmul_kernel(float *C, float *A, float *B) {\n",
    "\n",
    "    __shared__ float sA[block_size_y][block_size_x];\n",
    "    __shared__ float sB[block_size_y][block_size_x];\n",
    "\n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "    int x = blockIdx.x * block_size_x + tx;\n",
    "    int y = blockIdx.y * block_size_y + ty;\n",
    "\n",
    "    float sum = 0.0;\n",
    "    int k,kb;\n",
    "\n",
    "    for (k=0; k<WIDTH; k+=block_size_x) {\n",
    "        __synchthreads();\n",
    "        sA[ty][tx] = A[y*WIDTH+k+tx];\n",
    "        sB[ty][tx] = B[(k+ty)*WIDTH+x];\n",
    "        __synchthreads();\n",
    "\n",
    "        for (kb=0; kb<block_size_x; kb++) {\n",
    "            sum += sA[ty][kb] * sB[kb][tx];\n",
    "        }\n",
    "\n",
    "    }\n",
    "\n",
    "    C[y*WIDTH+x] = sum;\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: GeForce GTX TITAN X\n",
      "block_size_x=8, block_size_y=8, time=247.656115723\n",
      "block_size_x=16, block_size_y=16, time=184.494805908\n",
      "block_size_x=32, block_size_y=32, time=183.317108154\n",
      "best performing configuration: block_size_x=32, block_size_y=32, time=183.317108154\n"
     ]
    }
   ],
   "source": [
    "restrict = [\"block_size_x==block_size_y\"] \n",
    "\n",
    "results = kernel_tuner.tune_kernel(\"matmul_kernel\", \"matmul_data_reuse.cu\",\n",
    "                                   problem_size, args, tune_params, restrictions=restrict, answer=answer, atol=1e-3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have made matrix multiplication about three times faster now, which comes from much better memory use:\n",
    "\n",
    "![](Matmul-utilisation-with-data-reuse.png)\n",
    "\n",
    "The compute intensity has dropped slightly, because of the syncthread operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
